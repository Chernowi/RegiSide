# MAPPO Configuration for Enemy Defeat Only Rewards
# Tests pure enemy-defeat based reward structure

algorithm: "mappo"

# Environment configuration
env_config: "configs/env/env_enemy_defeat_only.yaml"

# MAPPO hyperparameters (optimized for enemy-defeat rewards)
mappo:
  total_timesteps: 5_000_000  # May need more training for sparse rewards
  num_steps: 256
  num_minibatches: 8
  update_epochs: 4

  learning_rate: 3.0e-4
  lr_schedule: "linear"
  anneal_lr: true

  gamma: 0.99
  gae_lambda: 0.95

  clip_eps: 0.2
  clip_vloss: true
  max_grad_norm: 0.5

  vf_coef: 0.5
  ent_coef: 0.01

  network:
    hidden_dims: [256, 256]
    activation: "relu"
    layer_norm: false
    use_orthogonal_init: true

  normalize_advantages: true
  normalize_values: false

# Logging configuration
logging:
  log_interval: 10
  save_interval: 100
  eval_interval: 50
  eval_episodes: 10

  use_tensorboard: true
  tensorboard_dir: "runs"

  verbose: true
  log_level: "INFO"

  experiment_name: "mappo_enemy_defeat_only"
  run_name: null
  tags: ["mappo", "regicide", "enemy-defeat-only", "sparse-rewards"]
  notes: "MAPPO with rewards only for enemy defeats (no time-based rewards)"

# Checkpointing
checkpoint:
  save_dir: "checkpoints"
  save_best: true
  save_last: true
  metric: "episode_return"
  mode: "max"

# Evaluation
evaluation:
  enabled: true
  deterministic: true
  num_episodes: 10
  render: false

# Reproducibility
seed: 42
deterministic_mode: false

# Hardware
device: "auto"
num_threads: 4

# Backend selection
backend: "pytorch"

# Advanced options
advanced:
  use_wandb: false
  wandb_project: "regicide-marl"
  wandb_entity: null